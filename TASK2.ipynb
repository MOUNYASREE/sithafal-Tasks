{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOrBiL9z+ncw1c4gcxpqEPy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MOUNYASREE/sithafal-Tasks/blob/main/TASK2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required libraries\n",
        "!pip install requests transformers faiss-cpu\n",
        "\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "import torch\n",
        "import faiss\n",
        "import numpy as np\n",
        "\n",
        "# Load Hugging Face model and tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "model = AutoModel.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "\n",
        "# Step 1: Crawl and Scrape\n",
        "def crawl_and_scrape(urls):\n",
        "    website_data = {}\n",
        "    for url in urls:\n",
        "        try:\n",
        "            response = requests.get(url)\n",
        "            response.raise_for_status()\n",
        "            soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "            text = \" \".join([p.get_text() for p in soup.find_all(\"p\")])\n",
        "            website_data[url] = text\n",
        "        except Exception as e:\n",
        "            print(f\"Error scraping {url}: {e}\")\n",
        "    return website_data\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E3V6MR6cmvKF",
        "outputId": "a2e15d29-ccd6-4dc1-db9f-9ac2d8d799ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.32.3)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.47.0)\n",
            "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.10/dist-packages (1.9.0.post1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2024.12.14)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.27.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (2024.10.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (4.12.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Chunk and Embed\n",
        "def chunk_and_embed(data, tokenizer, model, chunk_size=300):\n",
        "    chunks = []\n",
        "    embeddings = []\n",
        "    for url, content in data.items():\n",
        "        words = content.split()\n",
        "        for i in range(0, len(words), chunk_size):\n",
        "            chunk = \" \".join(words[i:i + chunk_size])\n",
        "            if len(chunk.strip()) > 0:\n",
        "                chunks.append((url, chunk.strip()))\n",
        "                tokens = tokenizer(chunk, return_tensors=\"pt\", truncation=True, padding=True)\n",
        "                with torch.no_grad():\n",
        "                    embedding = model(**tokens).pooler_output.squeeze().numpy()\n",
        "                    embeddings.append(embedding)\n",
        "    return chunks, np.array(embeddings)"
      ],
      "metadata": {
        "id": "YU9iWo7DmwaT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Step 3: Store in FAISS\n",
        "def store_embeddings(embeddings):\n",
        "    d = embeddings.shape[1]  # Dimension of embeddings\n",
        "    index = faiss.IndexFlatL2(d)\n",
        "    index.add(embeddings)\n",
        "    return index"
      ],
      "metadata": {
        "id": "6lDaMDdomwpJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: Query Handling\n",
        "def query_vector_database(query, index, tokenizer, model, chunks, top_k=5):\n",
        "    tokens = tokenizer(query, return_tensors=\"pt\", truncation=True, padding=True)\n",
        "    with torch.no_grad():\n",
        "        query_embedding = model(**tokens).pooler_output.numpy()\n",
        "    distances, indices = index.search(query_embedding, k=top_k)\n",
        "    results = [chunks[i] for i in indices[0] if i < len(chunks)]\n",
        "    return results"
      ],
      "metadata": {
        "id": "VF9MIVXMmw1H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Step 5: Generate Response\n",
        "def generate_response(results):\n",
        "    if not results:\n",
        "        return \"Sorry, I couldn't find any relevant information.\"\n",
        "    response = \"Based on your query, here are the results:\\n\\n\"\n",
        "    for url, text in results:\n",
        "        response += f\"URL: {url}\\nContent: {text[:200]}...\\n\\n\"\n",
        "    return response"
      ],
      "metadata": {
        "id": "JjuVXeTKm9t5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Main Function\n",
        "def main():\n",
        "    # Step 1: Define URLs\n",
        "    urls = [\n",
        "        \"https://www.uchicago.edu/\",\n",
        "        \"https://www.washington.edu/\",\n",
        "        \"https://www.stanford.edu/\",\n",
        "        \"https://und.edu/\",\n",
        "    ]\n",
        "\n",
        "    # Step 2: Crawl and scrape websites\n",
        "    print(\"Crawling and scraping websites...\")\n",
        "    website_data = crawl_and_scrape(urls)\n",
        "\n",
        "    # Step 3: Chunk and embed content\n",
        "    print(\"Chunking and embedding content...\")\n",
        "    chunks, embeddings = chunk_and_embed(website_data, tokenizer, model)\n",
        "\n",
        "    # Step 4: Store embeddings in FAISS\n",
        "    print(\"Storing embeddings in FAISS...\")\n",
        "    index = store_embeddings(embeddings)\n",
        "\n",
        "    # Step 5: Handle user queries\n",
        "    while True:\n",
        "        query = input(\"\\nEnter your query (or 'exit' to quit): \")\n",
        "        if query.lower() == \"exit\":\n",
        "            print(\"Exiting the program.\")\n",
        "            break\n",
        "        print(\"Searching content...\")\n",
        "        results = query_vector_database(query, index, tokenizer, model, chunks)\n",
        "        print(\"Generating response...\")\n",
        "        response = generate_response(results)\n",
        "        print(response)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1rnfwzlem9qV",
        "outputId": "b109b30f-5f7d-4bfe-9baf-83b0b29ecb0e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Crawling and scraping websites...\n",
            "Error scraping https://www.uchicago.edu/: 403 Client Error: Forbidden for url: https://www.uchicago.edu/\n",
            "Chunking and embedding content...\n",
            "Storing embeddings in FAISS...\n",
            "\n",
            "Enter your query (or 'exit' to quit): \"research opportunities at the University of Chicago\"?\n",
            "Searching content...\n",
            "Generating response...\n",
            "Based on your query, here are the results:\n",
            "\n",
            "URL: https://www.stanford.edu/\n",
            "Content: Other ways to search: Map Profiles Stanford Explore Stanford Stanford was founded almost 150 years ago on a bedrock of societal purpose. Our mission is to contribute to the world by educating students...\n",
            "\n",
            "URL: https://www.washington.edu/\n",
            "Content: UW astronomy undergrads are using cutting-edge coding skills to help scientists make the most of discoveries from a revolutionary new telescope. Read story Chris Mantegna, â21, is studying how pollu...\n",
            "\n",
            "URL: https://www.stanford.edu/\n",
            "Content: a vibrant community of creative and accomplished people from around the world A residential campus with diverse housing, exceptional dining, and over 600 student organizations Student Affairs A rich t...\n",
            "\n",
            "URL: https://und.edu/\n",
            "Content: The University of North Dakota is the state's oldest and largest university. We offer 225+ highly accredited on-campus and online degrees. Explore the causes and impact of criminal behavior and prepar...\n",
            "\n",
            "URL: https://und.edu/\n",
            "Content: The University of North Dakota is the state's oldest and largest university. We offer 225+ highly accredited on-campus and online degrees. Explore the causes and impact of criminal behavior and prepar...\n",
            "\n",
            "\n",
            "\n",
            "Enter your query (or 'exit' to quit): \"admissions requirements for undergraduates at Washington\"\n",
            "Searching content...\n",
            "Generating response...\n",
            "Based on your query, here are the results:\n",
            "\n",
            "URL: https://www.washington.edu/\n",
            "Content: UW astronomy undergrads are using cutting-edge coding skills to help scientists make the most of discoveries from a revolutionary new telescope. Read story Chris Mantegna, â21, is studying how pollu...\n",
            "\n",
            "URL: https://www.stanford.edu/\n",
            "Content: Other ways to search: Map Profiles Stanford Explore Stanford Stanford was founded almost 150 years ago on a bedrock of societal purpose. Our mission is to contribute to the world by educating students...\n",
            "\n",
            "URL: https://www.stanford.edu/\n",
            "Content: a vibrant community of creative and accomplished people from around the world A residential campus with diverse housing, exceptional dining, and over 600 student organizations Student Affairs A rich t...\n",
            "\n",
            "URL: https://und.edu/\n",
            "Content: The University of North Dakota is the state's oldest and largest university. We offer 225+ highly accredited on-campus and online degrees. Explore the causes and impact of criminal behavior and prepar...\n",
            "\n",
            "URL: https://und.edu/\n",
            "Content: The University of North Dakota is the state's oldest and largest university. We offer 225+ highly accredited on-campus and online degrees. Explore the causes and impact of criminal behavior and prepar...\n",
            "\n",
            "\n",
            "\n",
            "Enter your query (or 'exit' to quit): exit\n",
            "Exiting the program.\n"
          ]
        }
      ]
    }
  ]
}